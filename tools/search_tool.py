from typing import Optional, Dict, Any, Tuple
import arxiv
import requests
import os
import http.client
import json
from dotenv import load_dotenv
from langchain_community.utilities import GoogleSerperAPIWrapper
import re
from autogen_core.tools import FunctionTool

def get_arxiv_tool():
    return FunctionTool(
        func=search_arxiv,
        description="Search for papers on arXiv",
        strict=True,
    )
def get_semantic_scholar_tool():
    return FunctionTool(
        func=search_semantic_scholar,
        description="Search for papers on semantic scholar",
        strict=True,
    )
def get_search_google_scholar_tool():
    return FunctionTool(
        func=search_google_scholar,
        description="Search for papers on google",
        strict=True
    )


# arXivæ£€ç´¢å·¥å…·
load_dotenv()

"""
{
    "name": "å·¥å…·åç§°",       // å¯é€‰å€¼ï¼šsearch_arxiv
    "parameters": {
        "query": "ç ”ç©¶ä¸»é¢˜å…³é”®è¯", // å¿…é€‰å‚æ•°
        "max_results": 5
    }
}
"""


async def search_arxiv(
        query: str,

) -> Dict[str, Any]:
    """
    Search arXiv papers by query.

    Args:
        query: Search query string
        max_results: Maximum number of results to returnï¼Œdefault 5.coding format

    Returns:
        Dictionary containing search results
    """
    print(f"ğŸ“š è°ƒç”¨arXivå·¥å…·: query={query}")

    try:
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=5,
            sort_by=arxiv.SortCriterion.Relevance
        )

        results = []
        for paper in client.results(search):

            results.append({
                "title": paper.title,
                "authors": [author.name for author in paper.authors],
                "abstract": paper.summary[:500] + "..." if len(paper.summary) > 500 else paper.summary,  # é™åˆ¶æ‘˜è¦é•¿åº¦
                "year": paper.published.year,
                "citation_count": None,
                "pdf_url": paper.pdf_url,
                "source_url": paper.entry_id,
                "source": "arXiv"
            })

        print(f"âœ… arXivæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ç¯‡è®ºæ–‡")
        return {"source": "arXiv", "papers": results, "total_count": len(results)}

    except Exception as e:
        print(f"âŒ arXivæœç´¢å‡ºé”™: {e}")
        return {"source": "arXiv", "papers": [], "error": str(e)}

# Semantic Scholaræ£€ç´¢å·¥å…·

"""
Sementic Schoolar is Allen Institute for AI çš„å…è´¹æœç´¢å¼•æ“ï¼Œ
ç»“åˆäººå·¥æ™ºèƒ½ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæä¾›é«˜è´¨é‡çš„å­¦æœ¯å­¦æœ¯æ–‡çŒ®æ£€ç´¢ï¼Œå¦‚æœè¦ä½¿ç”¨apiï¼Œéœ€è¦ç”³è¯·api key
Academic Graph API
Recommendations API
Datasets API
ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒæ’åå™¨ï¼Œå®Œæˆè®ºæ–‡ç›¸å…³è¡Œæœç´¢å’Œè®ºæ–‡æ‰¹é‡ã€è¿™ä¸ªæ›´åŠ ä¸è€—è´¹æ—¶é—´ã€‘æœç´¢ç»ˆç«¯èŠ‚ç‚¹

æˆ‘å·²ç»æäº¤ç”³è¯·ï¼Œä½†æ˜¯ä¼°è®¡é€šè¿‡è¦ç­‰ä¸€æ®µæ—¶é—´
"""

"""
{
    "name": "å·¥å…·åç§°",       // å¯é€‰å€¼ï¼šsearch_semantic_scholar
    "parameters": {
        "query": "ç ”ç©¶ä¸»é¢˜å…³é”®è¯", // å¿…é€‰å‚æ•°
        "max_results": 10,        // å¯é€‰å‚æ•°ï¼ˆæ•´æ•°ç±»å‹ï¼‰
        "year_range": [2020, 2023] // å¯é€‰å‚æ•°ï¼ˆæ•´æ•°æ•°ç»„ï¼‰
    }
}
"""
async def search_semantic_scholar(
        query: str,
)->dict:
    url = "https://api.semanticscholar.org/graph/v1/paper/search"
    max_results = 5
    params = {
        "query": query,
        "limit": max_results,
        "fields": "title,authors,abstract,year,citationCount,url,pdfUrls"
    }

    headers = {
        "x-api-key": os.getenv("SEMANTIC_SCHOLAR_API_KEY")
    } if os.getenv("SEMANTIC_SCHOLAR_API_KEY") else {}

    response = requests.get(url, params=params, headers=headers)
    data = response.json()

    papers = []
    for item in data.get("data", []):
        papers.append({
            "title": item.get("title", ""),
            "authors": [author["name"] for author in item.get("authors", [])],
            "abstract": item.get("abstract", ""),
            "year": item.get("year"),
            "citation_count": item.get("citationCount", 0),
            "pdf_url": item.get("pdfUrls", [""])[0] if item.get("pdfUrls") else "",
            "source_url": item.get("url", ""),
            "source": "Semantic Scholar"
        })
    return {"source": "Semantic Scholar", "papers": papers}


"""
serperæ˜¯googleä¸€ä¸ªå¿«é€Ÿå“åº”å¹³å°
import http.client
import json

conn = http.client.HTTPSConnection("google.serper.dev")
payload = json.dumps({
  "q": "apple inc"
})
headers = {
  'X-API-KEY': '48b9aa9bf889cd6031dae46b74f9a9930558c35e',
  'Content-Type': 'application/json'
}
conn.request("POST", "/search", payload, headers)
res = conn.getresponse()
data = res.read()
print(data.decode("utf-8"))

"""


# Google Scholaræ£€ç´¢å·¥å…·
"""
{
    "name": "å·¥å…·åç§°",       // å¯é€‰å€¼ï¼šsearch_google_scholar
    "parameters": {
        "query": "éœ€è¦æœç´¢å…³é”®è¯", // å¿…é€‰å‚æ•°
    }
}
"""
async def search_google_scholar(
        query: str,
)->dict:
    serper = GoogleSerperAPIWrapper()
    results = serper.results(query)
    max_results = 5

    papers = []
    for item in results.get("organic", [])[:max_results]:
        # æå–å¹´ä»½ä¿¡æ¯
        year_match = re.search(r'(\d{4})', item.get("snippet", ""))
        year = int(year_match.group(1)) if year_match else None
        papers.append({
            "title": item.get("title", ""),
            "authors": [],  # Google Scholarä¸ç›´æ¥æä¾›ä½œè€…åˆ—è¡¨
            "abstract": item.get("snippet", ""),
            "year": year,
            "citation_count": None,  # éœ€è¦é¢å¤–è§£æ
            "pdf_url": "",
            "source_url": item.get("link", ""),
            "source": "Google Scholar"
        })
    return {"source": "Google Scholar", "papers": papers}
