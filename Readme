
## 1. Create a pyenv
//On Windows, change `python3` to `python` (if `python` is Python 3).
python3 -m venv .venv
//On Windows, change `bin` to `scripts`.
source .venv/bin/activate

## 2. pip necessary package

pip install -U "autogen-agentchat" "autogen-ext[openai,azure]"

pip install -U "autogen-agentchat"
//for openai model
pip install "autogen-ext[openai]"
//for Azure OpenAI and AAD certification
pip install "autogen-ext[azure]"

## 3.For different ai model

### for openai
[config]
from autogen_ext.models.openai import OpenAIChatCompletionClient
openai_model_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)

[client message]
from autogen_core.models import UserMessage
result = await openai_model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)
await openai_model_client.close()

我们暂时不考虑使用azure部署节点、以及身份验证等内容；



## 4. fastapi+agent for web service
pip install -U "autogen-agentchat" "autogen-ext[openai]" "fastapi" "uvicorn[standard]" "PyYAML"

### for claude
!pip install -U "autogen-ext[anthropic]"
from autogen_core.models import UserMessage
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
anthropic_client = AnthropicChatCompletionClient(model="claude-3-7-sonnet-20250219")
result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)
await anthropic_client.close()


### for ollama
pip install -U "autogen-ext[ollama]"
from autogen_core.models import UserMessage
from autogen_ext.models.ollama import OllamaChatCompletionClient
# Assuming your Ollama server is running locally on port 11434.
ollama_model_client = OllamaChatCompletionClient(model="llama3.2")
response = await ollama_model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(response)
await ollama_model_client.close()


### for gemini
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(
    model="gemini-1.5-flash-8b",
    # api_key="GEMINI_API_KEY",
)
response = await model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(response)
await model_client.close()

多模型实现【在discussion】阶段，在其他时候我们都使用一个主要的openai单模型


